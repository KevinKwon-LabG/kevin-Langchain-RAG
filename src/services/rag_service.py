import json
import logging
from typing import List, Dict, Any, Optional
import requests

from langchain.prompts import PromptTemplate
from langchain.schema import Document

from src.config.settings import settings
from src.services.document_service import document_service
from src.services.llm_decision_service import llm_decision_service
from src.services.websearch_service import websearch_service

logger = logging.getLogger(__name__)

class RAGService:
    def __init__(self):
        self.context_template = PromptTemplate(
            input_variables=["context", "question"],
            template="""
ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”. 
êµ¬ê¸€ ê²€ìƒ‰ ê²°ê³¼ê°€ ìˆë‹¤ë©´ ìµœì‹  ì •ë³´ë¥¼ ìš°ì„ ì ìœ¼ë¡œ ì°¸ê³ í•˜ì„¸ìš”.
ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ëŠ” ê²½ìš°, "ì»¨í…ìŠ¤íŠ¸ì—ì„œ í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.

ì»¨í…ìŠ¤íŠ¸:
{context}

ì§ˆë¬¸: {question}

ë‹µë³€:"""
        )

    def retrieve_context(self, query: str, top_k: int = 5) -> str:
        docs = document_service.search_documents(query, top_k=top_k)
        context = "\n".join([doc.page_content for doc in docs])
        return context

    def build_context_prompt(self, context: str, question: str) -> str:
        return self.context_template.format(context=context, question=question)

    def generate_response_with_rag(self, query: str, model: str, top_k: int = 5, system_prompt: Optional[str] = None) -> str:
        # 1. ê²€ìƒ‰ í•„ìš”ì„± íŒë‹¨
        needs_search = llm_decision_service.needs_web_search(query, model_name=model)
        web_context = ""
        if needs_search:
            # 2. êµ¬ê¸€ ê²€ìƒ‰
            web_context = websearch_service.search_web(query)
            logger.info(f"êµ¬ê¸€ ê²€ìƒ‰ ê²°ê³¼ ì‚¬ìš©: {len(web_context)} ë¬¸ì")
        # 3. ê¸°ì¡´ RAG context
        rag_context = self.retrieve_context(query, top_k=top_k)
        # 4. context í•©ì¹˜ê¸° (êµ¬ê¸€ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ìš°ì„  ë°°ì¹˜)
        context_parts = []
        if web_context:
            context_parts.append(f"ğŸŒ [ì‹¤ì‹œê°„ êµ¬ê¸€ ê²€ìƒ‰ ê²°ê³¼ - ìµœì‹  ì •ë³´]\n{web_context}")
        if rag_context:
            context_parts.append(f"ğŸ“„ [ë‚´ë¶€ ë¬¸ì„œ]\n{rag_context}")
        
        full_context = "\n\n".join(context_parts) if context_parts else "ê²€ìƒ‰ëœ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤."
        
        prompt = self.build_context_prompt(full_context, query)
        # 5. LLM í˜¸ì¶œ
        from langchain_community.llms import Ollama
        llm = Ollama(model=model)
        response = llm.invoke(prompt)
        return response

rag_service = RAGService() 