import json
import logging
from typing import List, Dict, Any, Optional
import requests

from langchain.prompts import PromptTemplate
from langchain.schema import Document

from src.config.settings import settings
from src.services.document_service import document_service
from src.services.llm_decision_service import llm_decision_service
from src.services.websearch_service import websearch_service
from src.utils.time_parser import time_parser

logger = logging.getLogger(__name__)

class RAGService:
    def __init__(self):
        self.context_template = PromptTemplate(
            input_variables=["context", "question", "time_context"],
            template="""
ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”. 
êµ¬ê¸€ ê²€ìƒ‰ ê²°ê³¼ê°€ ìˆë‹¤ë©´ ìµœì‹  ì •ë³´ë¥¼ ìš°ì„ ì ìœ¼ë¡œ ì°¸ê³ í•˜ì„¸ìš”.
ì‹œê°„ ì •ë³´ê°€ ì œê³µëœ ê²½ìš°, í•´ë‹¹ ì‹œê°„ì„ ê¸°ì¤€ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.
ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ëŠ” ê²½ìš°, "ì»¨í…ìŠ¤íŠ¸ì—ì„œ í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.

ì‹œê°„ ì»¨í…ìŠ¤íŠ¸: {time_context}

ì»¨í…ìŠ¤íŠ¸:
{context}

ì§ˆë¬¸: {question}

ë‹µë³€:"""
        )

    def retrieve_context(self, query: str, top_k: int = 5) -> str:
        docs = document_service.search_documents(query, top_k=top_k)
        context = "\n".join([doc.page_content for doc in docs])
        return context

    def build_context_prompt(self, context: str, question: str, time_context: str = "") -> str:
        return self.context_template.format(context=context, question=question, time_context=time_context)

    def generate_response_with_rag(self, query: str, model: str, top_k: int = 5, system_prompt: Optional[str] = None) -> str:
        # 0. ì‹œê°„ ì •ë³´ íŒŒì‹±
        time_info = time_parser.parse_time_expressions(query)
        time_context = time_parser.get_time_context(query)
        
        # 1. ê²€ìƒ‰ í•„ìš”ì„± íŒë‹¨ (ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ì ìœ¼ë¡œ ê²€ìƒ‰ ìˆ˜í–‰)
        needs_search = True
        try:
            needs_search = llm_decision_service.needs_web_search(query, model_name=model)
            logger.info(f"ê²€ìƒ‰ í•„ìš”ì„± íŒë‹¨: {needs_search}")
        except Exception as e:
            logger.warning(f"ê²€ìƒ‰ í•„ìš”ì„± íŒë‹¨ ì‹¤íŒ¨, ê¸°ë³¸ì ìœ¼ë¡œ ê²€ìƒ‰ ìˆ˜í–‰: {e}")
            needs_search = True
        
        web_context = ""
        if needs_search:
            # 2. êµ¬ê¸€ ê²€ìƒ‰
            try:
                web_context = websearch_service.search_web(query)
                logger.info(f"êµ¬ê¸€ ê²€ìƒ‰ ê²°ê³¼ ì‚¬ìš©: {len(web_context)} ë¬¸ì")
            except Exception as e:
                logger.error(f"êµ¬ê¸€ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
                web_context = ""
        
        # 3. ê¸°ì¡´ RAG context
        rag_context = self.retrieve_context(query, top_k=top_k)
        
        # 4. context í•©ì¹˜ê¸° (êµ¬ê¸€ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ìš°ì„  ë°°ì¹˜)
        context_parts = []
        if web_context:
            context_parts.append(f"ğŸŒ [ì‹¤ì‹œê°„ êµ¬ê¸€ ê²€ìƒ‰ ê²°ê³¼ - ìµœì‹  ì •ë³´]\n{web_context}")
        if rag_context:
            context_parts.append(f"ğŸ“„ [ë‚´ë¶€ ë¬¸ì„œ]\n{rag_context}")
        
        full_context = "\n\n".join(context_parts) if context_parts else "ê²€ìƒ‰ëœ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤."
        
        prompt = self.build_context_prompt(full_context, query, time_context)
        # 5. LLM í˜¸ì¶œ
        from langchain_community.llms import Ollama
        llm = Ollama(model=model)
        response = llm.invoke(prompt)
        return response

rag_service = RAGService() 