"""
ì±„íŒ… ê´€ë ¨ API ì—”ë“œí¬ì¸íŠ¸
Ollama ëª¨ë¸ê³¼ì˜ ëŒ€í™”, ì„¸ì…˜ ê´€ë¦¬ ë“±ì„ ì œê³µí•©ë‹ˆë‹¤.
"""

import logging
import json
import requests
from fastapi import APIRouter, HTTPException
from fastapi.responses import StreamingResponse
from typing import Dict, Any, Optional
from src.models.schemas import ChatRequest
from src.utils.session_manager import (
    get_or_create_session,
    add_message_to_session,
    get_session,
    delete_session,
    get_all_sessions,
    build_conversation_prompt
)
from src.services.langchain_decision_service import langchain_decision_service, DecisionCategory

from datetime import datetime

logger = logging.getLogger(__name__)
debug_logger = logging.getLogger("chat_debug")
debug_logger.setLevel(logging.DEBUG)
router = APIRouter(prefix="/api/chat", tags=["Chat"])

# Ollama ì„œë²„ ì„¤ì •
OLLAMA_BASE_URL = "http://1.237.52.240:11434"

# ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡
AVAILABLE_MODELS = [
    {"name": "gemma3:12b-it-qat", "size": "8.9 GB", "id": "5d4fa005e7bb"},
    {"name": "llama3.1:8b", "size": "4.9 GB", "id": "46e0c10c039e"},
    {"name": "llama3.2-vision:11b-instruct-q4_K_M", "size": "7.8 GB", "id": "6f2f9757ae97"},    
    {"name": "qwen3:14b-q8_0", "size": "15 GB", "id": "304bf7349c71"},
    {"name": "deepseek-r1:14b", "size": "9.0 GB", "id": "c333b7232bdb"},
    {"name": "deepseek-v2:16b-lite-chat-q8_0", "size": "16 GB", "id": "1d62ef756269"},        
]

@router.post("/")
async def chat(request: ChatRequest):
    """
    Ollama ëª¨ë¸ê³¼ ëŒ€í™”ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    
    Args:
        request: ì±„íŒ… ìš”ì²­ (ëª¨ë¸, ë©”ì‹œì§€, ì„¸ì…˜ ID ë“± í¬í•¨)
    
    Returns:
        ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ
    """
    try:
        debug_logger.debug(f"ğŸ’¬ ì±„íŒ… ìš”ì²­ ì‹œì‘ - ì„¸ì…˜: {request.session_id}, ëª¨ë¸: {request.model}")
        debug_logger.debug(f"ğŸ“ ì‚¬ìš©ì ë©”ì‹œì§€: {request.message[:100]}{'...' if len(request.message) > 100 else ''}")
        
        # ì„¸ì…˜ ê´€ë¦¬
        session = get_or_create_session(request.session_id)
        debug_logger.debug(f"ğŸ†” ì„¸ì…˜ ìƒì„±/ì¡°íšŒ ì™„ë£Œ: {session.session_id}")
        
        # Ollama ëª¨ë¸ ì‚¬ìš©
        # ì„¸ì…˜ì—ì„œ ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ê°€ì ¸ì™€ì„œ messages ë°°ì—´ êµ¬ì„±
        session_data = get_session(session.session_id)
        messages = []
        
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ê°€ ìˆìœ¼ë©´ ì¶”ê°€
        if request.system:
            messages.append({"role": "system", "content": request.system})
            debug_logger.debug(f"âš™ï¸ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì¶”ê°€: {request.system[:50]}...")
        
        # ì´ì „ ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¶”ê°€ (ìµœê·¼ 10ê°œ ë©”ì‹œì§€ë§Œ)
        if session_data and session_data.messages:
            history_count = len(session_data.messages[-10:])
            debug_logger.debug(f"ğŸ“š ëŒ€í™” íˆìŠ¤í† ë¦¬ {history_count}ê°œ ë©”ì‹œì§€ ì¶”ê°€")
            for message in session_data.messages[-10:]:
                messages.append({
                    "role": message.role,
                    "content": message.content
                })
        else:
            debug_logger.debug("ğŸ†• ìƒˆë¡œìš´ ëŒ€í™” ì‹œì‘ (íˆìŠ¤í† ë¦¬ ì—†ìŒ)")
        
        # í˜„ì¬ ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        messages.append({"role": "user", "content": request.message})
        debug_logger.debug(f"ğŸ‘¤ ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€ë¨ (ì´ {len(messages)}ê°œ ë©”ì‹œì§€)")
        
        # Ollama API í˜¸ì¶œ
        ollama_url = f"{OLLAMA_BASE_URL}/api/chat"
        payload = {
            "model": request.model,
            "messages": messages,
            "stream": True,
            "options": request.options or {}
        }
        debug_logger.debug(f"ğŸ¤– Ollama API í˜¸ì¶œ ì¤€ë¹„ - URL: {ollama_url}")
        debug_logger.debug(f"ğŸ“¦ í˜ì´ë¡œë“œ í¬ê¸°: {len(str(payload))} ë¬¸ì")
        
        def generate():
            """ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±"""
            try:
                debug_logger.debug("ğŸš€ Ollama API ìš”ì²­ ì‹œì‘...")
                response = requests.post(ollama_url, json=payload, stream=True)
                response.raise_for_status()
                debug_logger.debug("âœ… Ollama API ì—°ê²° ì„±ê³µ")
                
                full_response = ""
                chunk_count = 0
                for line in response.iter_lines():
                    if line:
                        data = json.loads(line.decode('utf-8'))
                        if data.get('done', False):
                            debug_logger.debug(f"âœ… ì‘ë‹µ ì™„ë£Œ - ì´ {chunk_count}ê°œ ì²­í¬, {len(full_response)} ë¬¸ì")
                            
                            # ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ì„¸ì…˜ì— ì¶”ê°€ (ì‘ë‹µ ì™„ë£Œ í›„)
                            add_message_to_session(session.session_id, "user", request.message, request.model)
                            debug_logger.debug("ğŸ’¾ ì‚¬ìš©ì ë©”ì‹œì§€ ì„¸ì…˜ì— ì €ì¥ë¨")
                            
                            # ì‘ë‹µ ì™„ë£Œ ì‹œ ì–´ì‹œìŠ¤í„´íŠ¸ ë©”ì‹œì§€ë¥¼ ì„¸ì…˜ì— ì¶”ê°€
                            add_message_to_session(
                                session.session_id, 
                                "assistant", 
                                full_response, 
                                request.model
                            )
                            debug_logger.debug("ğŸ’¾ ì–´ì‹œìŠ¤í„´íŠ¸ ë©”ì‹œì§€ ì„¸ì…˜ì— ì €ì¥ë¨")
                            break
                        
                        if 'message' in data and 'content' in data['message']:
                            chunk = data['message']['content']
                            full_response += chunk
                            chunk_count += 1
                            if chunk_count % 10 == 0:  # 10ê°œ ì²­í¬ë§ˆë‹¤ ë¡œê·¸
                                debug_logger.debug(f"ğŸ“¦ ì²­í¬ {chunk_count} ì²˜ë¦¬ ì¤‘... (í˜„ì¬ {len(full_response)} ë¬¸ì)")
                            yield f"data: {json.dumps({'response': chunk})}\n\n"
                        elif 'response' in data:
                            # í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•´ /api/generate í˜•ì‹ë„ ì§€ì›
                            chunk = data['response']
                            full_response += chunk
                            chunk_count += 1
                            if chunk_count % 10 == 0:  # 10ê°œ ì²­í¬ë§ˆë‹¤ ë¡œê·¸
                                debug_logger.debug(f"ğŸ“¦ ì²­í¬ {chunk_count} ì²˜ë¦¬ ì¤‘... (í˜„ì¬ {len(full_response)} ë¬¸ì)")
                            yield f"data: {json.dumps({'response': chunk})}\n\n"
                
                # ìµœì¢… ì‘ë‹µ ì „ì†¡
                debug_logger.debug("ğŸ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì™„ë£Œ")
                yield f"data: {json.dumps({'done': True})}\n\n"
                
            except Exception as e:
                debug_logger.error(f"âŒ Ollama API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
                logger.error(f"Ollama API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
                error_response = f"data: {json.dumps({'error': f'AI ëª¨ë¸ ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}'})}\n\n"
                yield error_response
        
        debug_logger.debug("ğŸ“¤ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ë°˜í™˜ ì‹œì‘")
        return StreamingResponse(generate(), media_type="text/plain")
        
    except Exception as e:
        debug_logger.error(f"âŒ ì±„íŒ… ìš”ì²­ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        logger.error(f"ì±„íŒ… ìš”ì²­ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=f"ì±„íŒ… ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

@router.post("/analyze-request")
async def analyze_chat_request(request: ChatRequest):
    """
    ì±„íŒ… ìš”ì²­ì„ ë¶„ì„í•˜ì—¬ ì ì ˆí•œ ì„œë¹„ìŠ¤ë¥¼ ê²°ì •í•©ë‹ˆë‹¤.
    Langchain decision ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    
    Args:
        request: ì±„íŒ… ìš”ì²­ (ëª¨ë¸, ë©”ì‹œì§€, ì„¸ì…˜ ID ë“± í¬í•¨)
    
    Returns:
        ë¶„ì„ ê²°ê³¼ (ì„œë¹„ìŠ¤ ê²°ì •, ì´ìœ , ì‹ ë¢°ë„ ë“±)
    
    Raises:
        HTTPException: ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí•œ ê²½ìš°
    """
    try:
        debug_logger.debug(f"ğŸ” ìš”ì²­ ë¶„ì„ ì‹œì‘ - ë©”ì‹œì§€: {request.message[:100]}{'...' if len(request.message) > 100 else ''}")
        
        # Langchain decision ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹¤ì œ ë¶„ì„ ìˆ˜í–‰
        debug_logger.debug("ğŸ¤– Langchain ì˜ì‚¬ê²°ì • ì„œë¹„ìŠ¤ í˜¸ì¶œ ì¤‘...")
        decision_result = await langchain_decision_service.classify_prompt(request.message)
        debug_logger.debug(f"ğŸ“Š ë¶„ë¥˜ ê²°ê³¼: {decision_result}")
        
        # ë¶„ë¥˜ ê²°ê³¼ì— ë”°ë¥¸ ì„œë¹„ìŠ¤ íƒ€ì… ê²°ì •
        service_type = "unknown"
        decision = "UNKNOWN"
        reason = "ë¶„ë¥˜ ì‹¤íŒ¨"
        confidence = 0.0
        
        if "ë‚ ì”¨ ì •ë³´" in decision_result:
            service_type = "weather_service"
            decision = "WEATHER_SERVICE"
            reason = "ë‚ ì”¨ ê´€ë ¨ ì •ë³´ ìš”ì²­ìœ¼ë¡œ íŒë‹¨ë¨"
            confidence = 0.9
        elif "í•œêµ­ ì£¼ì‹" in decision_result:
            service_type = "stock_service"
            decision = "STOCK_SERVICE"
            reason = "í•œêµ­ ì£¼ì‹ ì‹œì¥ ì •ë³´ ìš”ì²­ìœ¼ë¡œ íŒë‹¨ë¨"
            confidence = 0.9
        elif "ì›¹ ê²€ìƒ‰" in decision_result:
            service_type = "web_search_service"
            decision = "WEB_SEARCH_NEEDED"
            reason = "ìµœì‹  ì •ë³´ë‚˜ ì‹¤ì‹œê°„ ë°ì´í„°ê°€ í•„ìš”í•œ ì§ˆë¬¸ìœ¼ë¡œ íŒë‹¨ë¨"
            confidence = 0.8
        elif "ë°”ë¡œ ë‹µë³€" in decision_result:
            service_type = "direct_answer"
            decision = "DIRECT_ANSWER"
            reason = "AI ëª¨ë¸ì´ ë°”ë¡œ ë‹µë³€ ê°€ëŠ¥í•œ ì§ˆë¬¸ìœ¼ë¡œ íŒë‹¨ë¨"
            confidence = 0.95
        else:
            # ê¸°ë³¸ê°’
            service_type = "web_search_service"
            decision = "WEB_SEARCH_NEEDED"
            reason = "ë¶„ë¥˜ ê²°ê³¼ì— ë”°ë¼ ì›¹ ê²€ìƒ‰ì´ í•„ìš”í•  ê²ƒìœ¼ë¡œ íŒë‹¨ë¨"
            confidence = 0.7
        
        # ì‹ ë¢°ë„ ê¸°ë°˜ ì˜ì‚¬ê²°ì •: ì‹ ë¢°ë„ê°€ ë‚®ì€ ê²½ìš° ì›¹ ê²€ìƒ‰ìœ¼ë¡œ í´ë°±
        # ë‚ ì”¨ ì •ë³´, í•œêµ­ ì£¼ì‹ ì •ë³´ ë“± ëª¨ë“  ì„œë¹„ìŠ¤ íƒ€ì…ì— ì ìš©
        debug_logger.debug(f"ğŸ¯ ìµœì¢… ë¶„ë¥˜ ê²°ê³¼ - ì„œë¹„ìŠ¤: {service_type}, ê²°ì •: {decision}, ì‹ ë¢°ë„: {confidence}")
        
        if confidence < 0.5:
            original_service_type = service_type
            original_decision = decision
            original_reason = reason
            
            debug_logger.debug(f"âš ï¸ ì‹ ë¢°ë„ ë‚®ìŒ({confidence:.2f}), ì›¹ ê²€ìƒ‰ìœ¼ë¡œ í´ë°±")
            
            service_type = "web_search_service"
            decision = "WEB_SEARCH_NEEDED"
            reason = f"ë¶„ë¥˜ ì‹ ë¢°ë„ê°€ ë‚®ì•„({confidence:.2f}) ì›¹ ê²€ìƒ‰ì„ ê¶Œì¥í•©ë‹ˆë‹¤. ì›ë˜ ë¶„ë¥˜: {original_decision}"
            confidence = 0.5  # ì‹ ë¢°ë„ë¥¼ 0.5ë¡œ ì¡°ì •
        else:
            debug_logger.debug(f"âœ… ì‹ ë¢°ë„ ì¶©ë¶„({confidence:.2f}), ì›ë˜ ë¶„ë¥˜ ìœ ì§€")
        
        # ë¶„ì„ ê²°ê³¼ êµ¬ì„±
        result = {
            "chat_request": {
                "message": request.message,
                "model": request.model,
                "session_id": request.session_id
            },
            "analysis": {
                "decision": decision,
                "reason": reason,
                "confidence": confidence,
                "service_type": service_type,
                "decision_result": decision_result,
                "recommended_action": get_recommended_action(service_type)
            },
            "timestamp": datetime.now().isoformat()
        }
        
        debug_logger.debug(f"ğŸ“‹ ë¶„ì„ ì™„ë£Œ - ìµœì¢… ê²°ì •: {decision}, ì„œë¹„ìŠ¤: {service_type}")
        return result
        
    except Exception as e:
        logger.error(f"ìš”ì²­ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ ë¶„ì„ ê²°ê³¼ ë°˜í™˜
        return {
            "chat_request": {
                "message": request.message,
                "model": request.model,
                "session_id": request.session_id
            },
            "analysis": {
                "decision": "WEB_SEARCH_NEEDED",
                "reason": f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}",
                "confidence": 0.0,
                "service_type": "web_search_service",
                "decision_result": "ì •í™•í•œ ë‹µë³€ì„ ìœ„í•´ì„œëŠ” ì›¹ ê²€ìƒ‰ì´ í•„ìš”í•©ë‹ˆë‹¤.",
                "recommended_action": "ì›¹ ê²€ìƒ‰ ì„œë¹„ìŠ¤ ì‚¬ìš©ì„ ê¶Œì¥í•©ë‹ˆë‹¤."
            },
            "timestamp": datetime.now().isoformat()
        }


def get_recommended_action(service_type: str) -> str:
    """
    ì„œë¹„ìŠ¤ íƒ€ì…ì— ë”°ë¥¸ ê¶Œì¥ ì•¡ì…˜ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    
    Args:
        service_type: ì„œë¹„ìŠ¤ íƒ€ì…
        
    Returns:
        ê¶Œì¥ ì•¡ì…˜ ë¬¸ìì—´
    """
    actions = {
        "weather_service": "ë‚ ì”¨ API ì„œë¹„ìŠ¤ë¥¼ í˜¸ì¶œí•˜ì—¬ ì‹¤ì‹œê°„ ë‚ ì”¨ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.",
        "stock_service": "ì£¼ì‹ API ì„œë¹„ìŠ¤ë¥¼ í˜¸ì¶œí•˜ì—¬ ì‹¤ì‹œê°„ ì£¼ê°€ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.",
        "web_search_service": "ì›¹ ê²€ìƒ‰ ì„œë¹„ìŠ¤ë¥¼ í˜¸ì¶œí•˜ì—¬ ìµœì‹  ì •ë³´ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.",
        "direct_answer": "AI ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë°”ë¡œ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.",
        "unknown": "ì›¹ ê²€ìƒ‰ ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ì •ë³´ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤."
    }
    return actions.get(service_type, "ì›¹ ê²€ìƒ‰ ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ì •ë³´ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.")

@router.get("/models")
async def get_models():
    """
    ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤.
    
    Returns:
        ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡
    """
    try:
        # Ollama ì„œë²„ì—ì„œ ì‹¤ì œ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹œë„
        try:
            response = requests.get(f"{OLLAMA_BASE_URL}/api/tags")
            if response.status_code == 200:
                models_data = response.json()
                models = models_data.get('models', [])
                return {"models": models}
        except Exception as e:
            logger.warning(f"Ollama ì„œë²„ì—ì„œ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        
        # ê¸°ë³¸ ëª¨ë¸ ëª©ë¡ ë°˜í™˜
        return {"models": AVAILABLE_MODELS}
        
    except Exception as e:
        logger.error(f"ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail="ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")

@router.get("/sessions")
async def get_sessions():
    """
    ëª¨ë“  ì„¸ì…˜ ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.
    
    Returns:
        ì„¸ì…˜ ì •ë³´ ëª©ë¡
    """
    try:
        sessions = get_all_sessions()
        return {"sessions": sessions}
    except Exception as e:
        logger.error(f"ì„¸ì…˜ ëª©ë¡ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail="ì„¸ì…˜ ëª©ë¡ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")

@router.get("/sessions/{session_id}")
async def get_session_info(session_id: str):
    """
    íŠ¹ì • ì„¸ì…˜ ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.
    
    Args:
        session_id: ì¡°íšŒí•  ì„¸ì…˜ ID
    
    Returns:
        ì„¸ì…˜ ì •ë³´
    
    Raises:
        HTTPException: ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ëŠ” ê²½ìš°
    """
    try:
        session = get_session(session_id)
        if not session:
            raise HTTPException(status_code=404, detail="ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        return {"session": session}
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ì„¸ì…˜ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail="ì„¸ì…˜ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")

@router.delete("/sessions/{session_id}")
async def delete_session_endpoint(session_id: str):
    """
    ì„¸ì…˜ì„ ì‚­ì œí•©ë‹ˆë‹¤.
    
    Args:
        session_id: ì‚­ì œí•  ì„¸ì…˜ ID
    
    Returns:
        ì‚­ì œ ê²°ê³¼
    """
    try:
        success = delete_session(session_id)
        if success:
            return {"message": "ì„¸ì…˜ì´ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤."}
        else:
            raise HTTPException(status_code=404, detail="ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ì„¸ì…˜ ì‚­ì œ ì¤‘ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail="ì„¸ì…˜ ì‚­ì œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")

@router.post("/sessions")
async def create_session_endpoint():
    """
    ìƒˆë¡œìš´ ì„¸ì…˜ì„ ìƒì„±í•©ë‹ˆë‹¤.
    
    Returns:
        ìƒì„±ëœ ì„¸ì…˜ ì •ë³´
    """
    try:
        session = get_or_create_session()
        return {
            "session_id": session.session_id,
            "created_at": session.created_at,
            "message": "ìƒˆ ì„¸ì…˜ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤."
        }
    except Exception as e:
        logger.error(f"ì„¸ì…˜ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail="ì„¸ì…˜ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")

@router.get("/health")
async def health_check():
    """
    ì±„íŒ… ì„œë¹„ìŠ¤ ìƒíƒœë¥¼ í™•ì¸í•©ë‹ˆë‹¤.
    
    Returns:
        ì„œë¹„ìŠ¤ ìƒíƒœ ì •ë³´
    """
    try:
        # Ollama ì„œë²„ ì—°ê²° ìƒíƒœ í™•ì¸
        ollama_status = "unknown"
        try:
            response = requests.get(f"{OLLAMA_BASE_URL}/api/tags", timeout=5)
            ollama_status = "connected" if response.status_code == 200 else "error"
        except Exception:
            ollama_status = "disconnected"
        
        # ì„¸ì…˜ í†µê³„
        from src.utils.session_manager import get_session_stats
        session_stats = get_session_stats()
        
        health_info = {
            "status": "healthy",
            "timestamp": datetime.now().isoformat(),
            "ollama_server": ollama_status,
            "ollama_url": OLLAMA_BASE_URL,
            "session_stats": session_stats,
            "available_models": len(AVAILABLE_MODELS)
        }
        return health_info
    except Exception as e:
        logger.error(f"í—¬ìŠ¤ ì²´í¬ ì¤‘ ì˜¤ë¥˜: {e}")
        return {
            "status": "unhealthy",
            "timestamp": datetime.now().isoformat(),
            "error": str(e)
        }



 